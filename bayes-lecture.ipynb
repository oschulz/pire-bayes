{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "@static if !@isdefined Pkg\n",
    "    import Pkg\n",
    "    Pkg.instantiate()\n",
    "end\n",
    "basename(dirname(Pkg.project().path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7b7b849a-81ae-4985-92ec-78b66da75f5f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "    Bayesian Inference\n",
    "</h1>\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "    Oliver Schulz<br>\n",
    "    <small>\n",
    "        <a href=\"mailto:oschulz@mpp.mpg.de\" target=\"_blank\">oschulz@mpp.mpg.de</a>\n",
    "    </small>\n",
    "</p>\n",
    "\n",
    "<div style=\"margin-top:1em\">\n",
    "    <p style=\"text-align: center\">\n",
    "        <img src=\"images/logos/mpp-mpg-logo.svg\" style=\"height: 4em; display: inline-block; margin: 1em;\"/>\n",
    "        <img src=\"images/logos/bat-logo.svg\" style=\"height: 4em; display: inline-block; margin: 1em;\"/>\n",
    "        <img src=\"images/logos/julia-logo.svg\" style=\"height: 4em; display: inline-block; margin: 1em;\"/>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "    PIRE-GEMADARC summer school, Kingston 2022\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An story about statistics ...\n",
    "\n",
    "\n",
    "You go to a Covid rapid-testing center. The test they use has a sensitivity of 90% and a specificity of 98%. Your test is positive. How sure can you be that you're actually infected?\n",
    "\n",
    "\n",
    "#### You go to a frequentist stastician\n",
    "\n",
    "You: I have have this test result, am I actually infected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Frequentist: Yes!\n",
    "\n",
    "You: How sure are you of that?\n",
    "\n",
    "Frequentist: Your positive test is 45 times more likely to have come from an infected person than from an uninfected person.\n",
    "\n",
    "You: Does that mean I can be 90 % sure that I'm infected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Frequentist: Oh no! Not at all!\n",
    "\n",
    "You: Well, how sure can I be?\n",
    "\n",
    "Frequetist shrugs: That's the wrong question to ask.\n",
    "\n",
    "You: Aaaaaargh!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### So you go to a Bayesian statistician\n",
    "\n",
    "You: I have have this test result, am I actually infected?\n",
    "\n",
    "Bayesian: Depends ...\n",
    "\n",
    "You: On what?\n",
    "\n",
    "Bayesian: What percentage of the population is currently infected?\n",
    "\n",
    "You: Oh, I read about that in the media yesterday, experts think it's about 1.5 %\n",
    "\n",
    "Bayesian: Ok, based on that and your test result, I'm xx % sure you're infected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You: Hah, finally a clear answer!\n",
    "\n",
    "Bayesian: Wait!\n",
    "\n",
    "You: Huh?\n",
    "\n",
    "Bayesian: Why did you get tested?\n",
    "\n",
    "You: Oh, I was at a party a few days ago, and one of my friends there tested positive afterwards.\n",
    "\n",
    "Bayesian: What do you think the probability is that you got infected there?\n",
    "\n",
    "You: How should I know, that's why I got the test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian: Still, guess ...\n",
    "\n",
    "You: Uh, I don't know - let's say 10%\n",
    "\n",
    "Bayesian: Ok, then I'm yy % sure you're actually infected.\n",
    "\n",
    "You: But - what if I had said 25%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bayesian: Oh, then I'd be zz % sure you're actually infected.\n",
    "\n",
    "You: But that's just guesswork!\n",
    "\n",
    "Bayesian shrugs: That's how it works.\n",
    "\n",
    "You: Aaaaaargh!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So what can you you? Blame the test! Ernest Rutherford: \"If your experiment needs statistics, you ought to have done a better experiment.\" In Bayesian terms: If your data very informative, your conclusion will not be very sensitive to your prior.\n",
    "\n",
    "Unfortunately, we often can't do a better experiment. And so: Frequentist statistics will give you a clear answer, but you may not like the question. Bayesian statistics is more likely to answer your question, but you need to guess first. Both have important applications, and you must be careful in how to interpret the result - and ideally use a very educated guess (\"informed prior\").\n",
    "\n",
    "If you ask \"how sure are you about the result\" you're asking a Bayesian question. If you ask \"does that result explain the data\" you're asking a Frequentist question (\"Goodness of fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scientific process and Bayesian probability\n",
    "\n",
    "* Scientific learning process (simplified):  \n",
    "  knowledge before -> new data from experiment/observation -> knowledge after\n",
    "\n",
    "* Knowledge typically takes form of a model $M$ with parameters $\\theta$,  \n",
    "  e.g. Standard Model of particle physics with parameters like Higgs mass\n",
    "\n",
    "* Bayesian interpretation of probability:  \n",
    "  probability is a measure/quantification of belief\n",
    "\n",
    "* So $P(\\theta | M)$ quantifies our knowledge/belief regarding to  \n",
    "  which parameter values describe reality - assuming a specific model!\n",
    "\n",
    "* Question: Given what we believe we knew before (*prior* knowledge), $P(\\theta | M)$,  \n",
    "  and given some new data $D$,  \n",
    "  what is our belief afterwards (*posterior* knowledge), $P(\\theta|D, M)$?\n",
    "  \n",
    "* *Knowledge update*: $P(\\theta | M), D \\rightarrow P(\\theta|D, M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian inference\n",
    "\n",
    "* Given a model $M$ and parameters $\\theta$, observing some data $D$ is more or less likely:  \n",
    "  *likelihood function* $P(D|\\theta, M)$, often written as $\\mathcal{L}(\\theta | D)$ for a given model\n",
    "\n",
    "* We want to know $P(\\theta|D, M)$, but $P(\\theta|D, M) \\neq P(D|\\theta, M)$ ! (\"prosecutor's fallacy\")\n",
    "\n",
    "* Instead, Bayes' Theorem:\n",
    "  $$\n",
    "  P(\\theta|D, M) =\n",
    "  \\frac{P(D|\\theta, M)\\ P(\\theta | M)}{P(D | M)} =\n",
    "  \\frac{P(D|\\theta, M)\\ P(\\theta | M)}{\\int P(D|\\theta, M)\\ P(\\theta | M)\\ \\mathrm{d}\\theta}\n",
    "  $$\n",
    "\n",
    "* $P(D|\\theta, M) P(\\theta, M)$: Non-normalized posterior (sometimes just called posterior),  \n",
    "  is product of *likelihood* $P(D|\\theta, M)$ and *prior* $P(\\theta, M)$\n",
    "\n",
    "* $P(D | M) = \\int P(D|\\theta, M)\\ P(\\theta|M)\\ d\\theta$: Normalization constant,  \n",
    "  often termed *evidence* or *marginal likelihood*, only depends on model $M$\n",
    "\n",
    "* Convenient: Don't need evidence $P(D | M)$ (constant, but hard to compute)  \n",
    "  to get relative posterior probability of model parameters\n",
    "\n",
    "* *Can't* get posterior $P(\\theta | D, M)$ from data *without* stating prior $P(\\theta | M)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian model comparison\n",
    "\n",
    "* Often several competing models $M_1$, $M_2$, ...,  \n",
    "  e.g. Standard Model with massless $\\nu$ (19 parameters)  \n",
    "  vs. Extended Standard Model with massive $\\nu$, etc. (up to 26 parameters)\n",
    "\n",
    "* Given two models $M_1$ and $M_2$, some new data $D$ and prior belief $P(M_i)$:\n",
    "\n",
    "  $$\n",
    "  P(M_i|D) =\n",
    "  \\frac{P(D|M_i) P(M_i)}{P(D)}, \\quad  P(D) = 1\n",
    "  $$\n",
    "  \n",
    "  with (law of total probability)\n",
    "  \n",
    "  $$P(D|M_i) = \\int P(D|\\theta, M_i) P(\\theta | M_i)\\ \\mathrm{d}\\theta$$\n",
    "\n",
    "* So *probability ratio* $K$ (called *Bayes factor*) between the models:\n",
    "\n",
    "  $$\n",
    "  K = \\frac{P(M_1|D)}{P(M_2|D)} =\n",
    "  \\frac{P(D|M_1)\\ P(M_1)}{P(D|M_2)\\ P(M_2)}\n",
    "  $$\n",
    "\n",
    "* Problem: Integral $P(D|M_i)$ (*evidence*) typically very hard to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior space exploration via sampling\n",
    "\n",
    "* Likelihood $P(D|\\theta, M)$ usually complex\n",
    "\n",
    "* Single evaluation of $P(D|\\theta, M)$ may be fast,  \n",
    "  or may involve details simulation of physics processes\n",
    "\n",
    "* How do we find $\\theta$ that maximizes $P(\\theta|D, M)$ (\"best fit\")?\n",
    "\n",
    "* How do we find credible region(s) for $\\theta$  \n",
    "  (e.g. regions that contain 68% / 95% / 99% of probability mass)?\n",
    "\n",
    "* MCMC (see Lectures 6 and 7) allows us to draw samples from (theoretically)  \n",
    "  any probability density / distribution (need not be normalized)!\n",
    "\n",
    "* So: Draw sample from posterior, then analyse samples!\n",
    "\n",
    "* But: When are we done sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Metropolis-Hastings convergence behavior\n",
    "\n",
    "* http://chi-feng.github.io/mcmc-demo/app.html, banana function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MCMC convergence and effective number of samples\n",
    "\n",
    "* Need to sample long enough for Markov chain(s) to\n",
    "    * reach stationary distribution (also called invariant distribution or typical set),  \n",
    "      discard these initial samples\n",
    "    * travel around many times (*mixing*)\n",
    "\n",
    "* Run several independent Markov chains, wait until converged to same distribution\n",
    "\n",
    "* [Gelman, Rubin (1992)](https://projecteuclid.org/euclid.ss/1177011136) diagnostic -\n",
    "  define potential scale reduction factor (PSRF) $\\hat{R}_k$,  \n",
    "  given samples $\\theta_{jki}$ of parameters $k = 1 \\ldots n$ from chain $j$:\n",
    "  $$\n",
    "  W_k = \\mathrm{mean}_j\\left(\\mathrm{var}_i(\\theta_{jki})\\right), \\quad\n",
    "  B_k = \\mathrm{var}_j\\left(\\mathrm{mean}_i(\\theta_{jki})\\right), \\quad \\hat{R}_k = \\sqrt{\\frac{(1-1/n)W_k + B_k}{W_k}}\n",
    "  $$\n",
    "\n",
    "* Convergenge Criterion: $\\hat{R}_k < 1.1$ (or even much stricter) for all parameters\n",
    "\n",
    "* Many more and advanced convergence criteria developed over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Effective number of samples\n",
    "\n",
    "* Disadvantage of MCMC: Samples are correlated - how many samples did we *really* get, effectively?\n",
    "\n",
    "* Important quantity: Integrated autocorrellation time $\\hat{\\tau}_k$,  \n",
    "  estimated from normalized autocorrelation function $\\hat{\\rho}_k(\\tau)$\n",
    "\n",
    "  $$\n",
    "  \\hat{\\tau}_k = 1 + 2\\ \\sum_{\\tau = 1}^{\\infty} \\hat{\\rho}_k(\\tau), \\quad\n",
    "  \\hat{\\rho}_k(\\tau) = \\frac{\\hat{c}_k(\\tau)}{\\hat{c}_k(0)}, \\quad\n",
    "  \\hat{c}_k(\\tau) = \\frac{1}{N - \\tau} \\sum_{n = 1}^{N - \\tau} (\\theta_{k,i} - \\hat{\\theta}_k)\\ (\\theta_{k,i + \\tau} - \\hat{\\theta}_k)\n",
    "  $$\n",
    "  \n",
    "* Tricky: Can't run sum over all lags $\\tau$, due to noise -> heuristic cut-off (*initial sequence estimators*)\n",
    "\n",
    "* Given $n$ samples, effective sample size $\\mathrm{ESS}_k$ (per parameter) is\n",
    "  $$\\mathrm{ESS}_k = \\frac{n}{\\hat{\\tau}_k}$$\n",
    "\n",
    "* $\\mathrm{ESS}_k$ provides basis for additional convergence criteria\n",
    "\n",
    "* Variance of sample-mean estimators of $f(\\theta)$ scales with $1 / \\mathrm{ESS}_f$ not with $1 / n$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bayesian Analysis Toolkit (BAT):  \n",
    "\n",
    "* BAT is a software package for Bayesian inference, model comparison and more\n",
    "\n",
    "* Functionalities: \n",
    "    * Posterior density exploration via Markov chain Monte-Carlo (MCMC)\n",
    "    * Mode finding (*MAP*, maximum a-posteriori)\n",
    "    * Marginalization and determination of credible regions\n",
    "    * Evidence calculation (integral of non-normalized posterior)\n",
    "    * User-friendly plotting and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Walk through BAT tutorial\n",
    "\n",
    "* https://bat.github.io/BAT.jl/dev/installation/\n",
    "\n",
    "* https://bat.github.io/BAT.jl/dev/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems with random-walk Metropolis-Hastings\n",
    "\n",
    "* Random-walk Metropolis-Hastings robust workhorse of MCMC,  \n",
    "  see Lectures 6 and 7. But ..\n",
    "\n",
    "* In high-dimensional spaces, almost all phase space at edges/corners:  \n",
    "  if chain starts away from typical set (very likely),  \n",
    "  random walk won't find it: one right direction to go, $2^n -1$ wrong ones\n",
    "\n",
    "* Need very short proposal jumps to keep acceptance ratio high\n",
    "\n",
    "* Results in very long convergence and mixing times\n",
    "\n",
    "* In very high dimensions, random walk will basically never converge\n",
    "\n",
    "* Need something to guide us in the right direction  \n",
    "  (guide MCMC to - and keep on - *typical set*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Metropolis-Hastings in trouble\n",
    "\n",
    "* http://chi-feng.github.io/mcmc-demo/app.html, donut function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "`n`-dimensional standard multivariate normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using Plots, Distributions, LinearAlgebra, ArraysOfArrays\n",
    "\n",
    "function curse_of_dimensionality_plot(dlist::AbstractArray{<:Integer})\n",
    "    ndim_gauss_r(n::Integer) = norm.(nestedview(rand(MvNormal(float.(I(n))), 10^6)))\n",
    "    mkhist(n) = stephist!(ndim_gauss_r(n), normalize = true, label = \"n = $n\", nbins = 0:0.05:10)\n",
    "    plt = plot(xlabel = \"distance from mode\", ylabel = \"probability mass\")\n",
    "    mkhist.(dlist)\n",
    "    plt\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curse_of_dimensionality_plot([1, 2, 5, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In higher dimensions, almost zero probability mass at mode,  \n",
    "  large mass at vanishing posterior values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hamiltionan Monte Carlo (HMC)\n",
    "\n",
    "* Want to sample target probability density $\\pi(q)$ on space of N-dim parameter vectors $q$\n",
    "\n",
    "* Idea: Expand $N$-dimensional parameter space to $2 N$ by adding vectors $p$,  \n",
    "  $$q \\rightarrow (q, p) \\qquad$$\n",
    "\n",
    "* Interpret $q$ as position, add momentum vector $p$, with *canonical distribution*:\n",
    "  $$\\pi(q, p) = e^{- H(q, p)}$$\n",
    "\n",
    "* Choose $\\pi(q, p)$ to be a conditional distribution\n",
    "  $$\\pi(q, p) = \\pi(p | q)\\ \\pi(q)$$\n",
    "\n",
    "* Now $\\pi(q)$ can be recovered by marginalizing over $p$, and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(q, p) \n",
    "&= - \\log \\pi(q, p) \\\\\n",
    "&= - \\log \\pi(p | q) - \\log \\pi(q) \\\\\n",
    "&\\equiv K(p,q) + V(q)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Physics interpretation: kinetic energy $K(p)$ and potential energy $V(q)$ with *mass matrix* $M$\n",
    "\n",
    "$$\n",
    "K(p) = p^T M^{-1} p\\ /\\ 2 \\qquad\n",
    "V(q) = - \\log \\pi(q)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMC sampling cycle\n",
    "\n",
    "1) Sample $N$-dimensional momentum vector $p$ from conditional distribution\n",
    "$$p \\sim \\pi(p|q)$$\n",
    "\n",
    "2) Expand $N$-dimensional parameter space to $2 N$\n",
    "\n",
    "$$q \\rightarrow (q, p)$$\n",
    "\n",
    "3) Integrate Hamiltonian equations of motion (numerically) for a time $t$:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} q_i }{\\mathrm{d} t } =\n",
    "\\frac{\\partial H}{\\partial p_i} =\n",
    "\\frac{\\partial K}{\\partial p_i} =\n",
    "[M^{-1} p]_i, \\qquad\n",
    "\\frac{\\mathrm{d} p_i }{\\mathrm{d} t } =\n",
    "-\\frac{\\partial H}{\\partial q_i} =\n",
    "- \\frac{\\partial V}{\\partial q_i} =\n",
    "- \\frac{\\partial \\log \\pi(q)}{\\partial q_i}\n",
    "$$\n",
    "\n",
    "$$(q, p) \\xrightarrow{t} (q^*,p^*)$$\n",
    "\n",
    "3) Return to target parameter space by simple projection (marginalizing $p_t$ out):\n",
    "\n",
    "$$(q^*,p^*) \\rightarrow q^*$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMC sampling cycle, cont.\n",
    "\n",
    "4) Metropolis step: accept/reject proposed new $q^*$ with probability\n",
    "   $$\n",
    "   a(q^*, -p^* | q, p) = min\\Big[ 1,\\, \\exp\\left(-H(q^*,p^*)+H(q,p)\\right) \\Big]\n",
    "   $$\n",
    "   since Hamiltonian dynamics reversible and phase-space preserving (Liouville).\n",
    "\n",
    "   Also $\\partial H / \\partial t = 0$ (energy conservation) $ \\rightarrow a = 1$ (theoretically!)\n",
    "\n",
    "   In practice, no perfect energy conservation due to numerical errors:  \n",
    "   use symplectic, time-reversibile integrator to maximize $a$,  \n",
    "   e.g. [Leapfrog method](https://en.wikipedia.org/wiki/Leapfrog_integration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Hamiltonian Monte Carlo\n",
    "\n",
    "* http://chi-feng.github.io/mcmc-demo/app.html, donut function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMC advantages and disadvantages\n",
    "\n",
    "* Pro: HMC can achieve acceptance ratios >90%\n",
    "* Con: Numerical integration evaluates targes density many times\n",
    "* Pro: $\\mathrm{d} q_i\\ /\\ \\mathrm{d} t = - \\partial \\log \\pi(q)\\ /\\ \\partial q_i$\n",
    "  guides proposal in the right direction\n",
    "* Con: First need to have that gradient of $\\log\\ \\pi(q)$\n",
    "* Tuning problem: Find suitable mass matrix $M$ and integration time $t$:  \n",
    "  $M$ effectively transforms the parameter space,  \n",
    "  $t$ must neither be too short or too long,  \n",
    "  many ways of optimizing automatically\n",
    "* Note: Gradient-based algorithms usually perform better,  \n",
    "  only viable choice in very high-dimensional spaces\n",
    "* Automatic differentiation can often get us that gradient\n",
    "\n",
    "*For details, see R. Neil (2012) [[arXiv 1206.1901]](https://arxiv.org/abs/1206.1901) and M. Betancourt (2017) [[arXiv 1701.02434]](https://arxiv.org/abs/1701.02434).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward mode automatic differentiation with dual numbers\n",
    "\n",
    "* Performs a Jacobian-vector-product: $J v$\n",
    "* Replace every variable/parameter $x$ by $x + x'\\varepsilon$, where $x'$ is a derivative/gradient and $\\varepsilon$ is a nilpotent infinitesimal with $\\varepsilon^2 = 0$:\n",
    "\n",
    "$$(a + a'\\varepsilon) + (b + b'\\varepsilon) = (a + b) + (a' + b')\\varepsilon$$\n",
    "\n",
    "$$(a + a'\\varepsilon) \\cdot (b + b'\\varepsilon) = ab + ab'\\varepsilon + ba'\\varepsilon + a'b'\\varepsilon^2 = ab + (a b' + ba')\\varepsilon$$\n",
    "\n",
    "* Traverses chain rule from inside to outside\n",
    "* Implemented as Julia package [\"ForwardDiff.jl\"](https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "* Defines dual numbers, adds methods for dual numbers to all primitive operations (exponentials, logarithms, trigonometry, etc.)\n",
    "* Only possible due to multiple dispatch\n",
    "* Users can add custom/optimized derivatives/gradients for their own functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reverse mode automatic differentiation\n",
    "\n",
    "* Performs a vector-Jacobian-product, resp. adjoint-Jacobian-vector-product: $v^* J$ resp. $J^* v$\n",
    "* Traverses chain rule from outside to inside, based on pullback functions $\\mathcal{B}$.\n",
    "* For $\\ell(x) = f(g(x))$:\n",
    "\n",
    "$$\n",
    "\\bar{a} \\equiv \\frac{\\partial \\ell}{\\partial a}, \\quad\n",
    "\\bar{\\ell} = \\frac{\\partial \\ell}{\\partial \\ell} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{x} =\n",
    "\\frac{\\partial \\ell}{\\partial x} =\n",
    "\\frac{\\partial \\ell}{\\partial g} \\frac{\\partial g}{\\partial x} =\n",
    "\\bar{g}\\ \\frac{\\partial g}{\\partial x} =\n",
    "\\mathcal{B}_g(\\bar{g}) =\n",
    "\\mathcal{B}_g(\\mathcal{B}_f(\\bar{\\ell})) =\n",
    "\\mathcal{B}_g(\\mathcal{B}_f(1)),\n",
    "$$\n",
    "\n",
    "* Needs to build a graph/tape of programs to be differentiated\n",
    "* Julia package [\"Zygote.jl\"](https://github.com/FluxML/Zygote.jl) implements source-to-source reverse mode auto-diff\n",
    "* Transforms Julia's [SSA-form intermediate representation](https://docs.julialang.org/en/v1/devdocs/ssair/)\n",
    "* Only possible due to Julia's meta-programming capabilities\n",
    "* Users can add custom/optimized derivatives/gradients  \n",
    "  by providing custom adjoints via pullback functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mixed-Mode automatic differentiation\n",
    "\n",
    "* Forward mode auto-diff (e.g. ForwardDiff.jl) faster if few parameters,  \n",
    "  but many computational steps\n",
    "\n",
    "* Reverse mode auto-diff (e.g. Zygote.jl) faster if many parameters,  \n",
    "  but few computational steps.\n",
    "\n",
    "* Mixed-Mode automatic differentiation:  \n",
    "  Use forward mode for complex inner functions with few parameters,  \n",
    "  use reverse mode in outer loops with many parameters.  \n",
    "  Useful if likelihood (partially) factorizes in terms of parameters,  \n",
    "  e.g. parameters per pixel in image data, etc.\n",
    "\n",
    "* Zygote.jl supports mixed-mode AD via `Zygote.forwarddiff` hints  \n",
    "  using dual numbers *and* IR source code transformation  \n",
    "  (using ForwardDiff.jl internally)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Julia auto-diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: Simple HMC example in BAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other MCMC algorithms\n",
    "\n",
    "* Many, many MCMC algorithms, some variants of Metropolis-Hastings, some completely different.\n",
    "\n",
    "* Also quite popular: Nested sampling algorithms  \n",
    "  (handle multivariate posteriors well)\n",
    "\n",
    "* Demo: http://chi-feng.github.io/mcmc-demo/app.html, multimodal function\n",
    "\n",
    "* Many implementations of different algorithms in many languages,  \n",
    "  e.g.\n",
    "  [PyMC3](https://docs.pymc.io/),\n",
    "  [emcee](https://emcee.readthedocs.io/en/stable/),\n",
    "  [DNest4](https://github.com/eggplantbren/DNest4),\n",
    "  [dynesty](https://dynesty.readthedocs.io/en/latest/),\n",
    "  ..."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "none"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
